# @package _global_
# Changes specified in this config should be interpreted as relative to the _global_ package.
defaults:
  - override /algorithm: sac

n_epochs: 200

# The number of training steps after which to evaluate the policy.
eval_after_n_steps: 2000

# The number of testing rollouts.
n_test_rollouts: 10

# The n last epochs over which to average for determining early stopping condition.
early_stop_last_n: 3

# The data column on which early stopping is based.
early_stop_data_column: 'eval/success_rate'

env: 'FetchPickAndPlace-v2'

algorithm:
  replay_buffer_class: HerReplayBuffer
  buffer_size: 1000000
  batch_size: 256
  policy_kwargs:
    net_arch:
      - 256
      - 256
      - 256
  replay_buffer_kwargs:
    n_sampled_goal: 4
    goal_selection_strategy: 'future'

hydra:
  sweeper:
    study_name: sac_her_FetchPickAndPlace
    max_trials: 32
    n_jobs: 8
    direction: maximize
    max_duration_minutes: 36000
    min_trials_per_param: 2
    max_trials_per_param: 3
    search_space:
      ++algorithm.learning_rate:
        type: float
        low: 0.0001
        high: 0.005
        log: true
      ++algorithm.gamma:
        type: float
        low: 0.9
        high: 0.99
        log: true
      ++algorithm.tau:
        type: float
        low: 0.005
        high: 0.1
        log: true
      ++algorithm.policy_kwargs.n_critics:
        type: int
        low: 1
        high: 4
